seed: 42
log_level: info
output_dir: ./benchmark_output/
write_json_trace: true
write_chrome_trace: false
write_metrics: true
enable_profiling: false

cluster:
  num_replicas: 1

model:
  name: meta-llama/Llama-2-70b-hf
  tokenizer: meta-llama/Llama-2-70b-hf
  # name: codellama/CodeLlama-34b-Instruct-hf
  # tokenizer: codellama/CodeLlama-34b-Instruct-hf
  tensor_parallel_degree: 8
  num_layers: 80

request_generator:
  provider: synthetic
  max_tokens: 4096

synthetic_request_generator:
  length_provider: trace
  interval_provider: static
  min_tokens: 1024
  prefill_to_decode_ratio: 10
  num_requests: 1000

trace_request_generator:
  trace_file: ./data/processed_traces/sydney_enterprise.csv
  date: '2023-08-21'
  prefill_scale_factor: 0.3
  decode_scale_factor:  1
  time_scale_factor: 0.04

# Config for synthetic trace generator
trace_request_length_generator:
  trace_file: ./data/processed_traces/lmsys_chat_1m_conversation_stats_llama2_tokenizer.csv
  prefill_scale_factor: 1
  decode_scale_factor:  1

trace_request_interval_generator:
  trace_file: ./data/processed_traces/AzureFunctionsInvocationTraceForTwoWeeksJan2021Processed.csv
  start_time: "1970-01-04 12:00:00"
  end_time: "1970-01-04 15:00:00"
  time_scale_factor: 0.3

poisson_request_interval_generator:
  qps: 16

gamma_request_interval_generator:
  cv: 0.5
  qps: 0.2

zipf_request_length_generator:
  theta: 0.4
  scramble: false

replica_scheduler:
  provider: vllm
  max_batch_size: 16

sarathi_scheduler:
  chunk_size: 512
  enable_rolling_prefills: true
  prefill_fitting_tolerance: 0.2

vllm_scheduler:
  max_tokens_in_batch: 4096

metrics_store:
  wandb_project: "llm-simulator"
  wandb_group: "vllm-benchmark-test-op-metrics-a100-1000-profiling-final"
  wandb_run_name: ""
  subsamples: 500
  save_table_to_wandb: false
  enable_op_level_metrics: true
  enable_cpu_op_level_metrics: true
  enable_high_level_cuda_metrics: false
  enable_request_outputs: false
